{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cindy\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NARRE\n",
    "@author:\n",
    "Chong Chen (cstchenc@163.com)\n",
    "\n",
    "@ created:\n",
    "27/8/2017\n",
    "@references:\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import datetime\n",
    "import NARRE\n",
    "import sys\n",
    "\n",
    "# NOTE: They did not provide data/google.bin (pretrained word2vec) so I am setting it to None\n",
    "#tf.flags.DEFINE_string(\"word2vec\", \"../data/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "tf.flags.DEFINE_string(\"word2vec\", None, \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "tf.flags.DEFINE_string(\"valid_data\",\"../data/music.test\", \" Data for validation\")\n",
    "tf.flags.DEFINE_string(\"para_data\", \"../data/music.para\", \"Data parameters\")\n",
    "tf.flags.DEFINE_string(\"train_data\", \"../data/music.train\", \"Data for training\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_string(\"word2vec\", \"./data/rt-polaritydata/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 50, \"Dimensionality of character embedding \")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3\", \"Comma-separated filter sizes \")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability \")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.001, \"L2 regularizaion lambda\")\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 100, \"Batch Size \")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 40, \"Number of training epochs \")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(u_batch, i_batch, uid, iid, reuid, reiid, y_batch,batch_num):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_reuid: reuid,\n",
    "        deep.input_reiid: reiid,\n",
    "        deep.drop0: 0.8,\n",
    "\n",
    "        deep.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "    }\n",
    "    _, step, loss, accuracy, mae, u_a, i_a, fm = sess.run(\n",
    "        [train_op, global_step, deep.loss, deep.accuracy, deep.mae, deep.u_a, deep.i_a, deep.score],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    #print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, accuracy, mae))\n",
    "    return accuracy, mae, u_a, i_a, fm\n",
    "\n",
    "\n",
    "def dev_step(u_batch, i_batch, uid, iid, reuid, reiid, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.input_reuid: reuid,\n",
    "        deep.input_reiid: reiid,\n",
    "        deep.drop0: 1.0,\n",
    "        deep.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, loss, accuracy, mae = sess.run(\n",
    "        [global_step, deep.loss, deep.accuracy, deep.mae],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    # print(\"{}: step{}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, step, loss, accuracy, mae))\n",
    "\n",
    "    return [loss, accuracy, mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_word2vec(vocabulary):\n",
    "    # initial matrix with random uniform\n",
    "    u = 0\n",
    "    initW = np.random.uniform(-1.0, 1.0, (len(vocabulary), FLAGS.embedding_dim))\n",
    "    # load any vectors from the word2vec\n",
    "    print(\"Load word2vec u file {}\\n\".format(FLAGS.word2vec))\n",
    "    with open(FLAGS.word2vec, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            idx = 0\n",
    "\n",
    "            if word in vocabulary:\n",
    "                u = u + 1\n",
    "                idx = vocabulary[word]\n",
    "                initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return initW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_glove(vocabulary, glove_file, vocab_size=400000):\n",
    "    \"\"\"\n",
    "    Use pre-trained GloVe embedding as alternative to word2vec.\n",
    "    Load in file with specified vocab size and dimensional embedding size.\n",
    "    If word exists in user/item vocabulary, replace the row with the values.\n",
    "    \"\"\"\n",
    "    # initial matrix with random uniform\n",
    "    u = 0\n",
    "    initW = np.random.uniform(-1.0, 1.0, (len(vocabulary), FLAGS.embedding_dim))\n",
    "    \n",
    "    print(\"Load glove u file {}\\n\".format(glove_file))\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i in range(vocab_size):\n",
    "            line = f.readline()\n",
    "            word_embed = line.split()\n",
    "            word = word_embed[0]\n",
    "            embed = [float(x) for x in word_embed[1:]]\n",
    "            idx = 0\n",
    "\n",
    "            if word in vocabulary:\n",
    "                u = u + 1\n",
    "                idx = vocabulary[word]\n",
    "                initW[idx] = embed\n",
    "    return initW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying GloVe: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_file = \"../../glove.6B.50d.txt\" \n",
    "use_glove = True # turn on if use glove instead of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(FLAGS.para_data, 'rb')\n",
    "\n",
    "para = pickle.load(pkl_file)\n",
    "user_num = para['user_num']\n",
    "item_num = para['item_num']\n",
    "review_num_u = para['review_num_u']\n",
    "review_num_i = para['review_num_i']\n",
    "review_len_u = para['review_len_u']\n",
    "review_len_i = para['review_len_i']\n",
    "vocabulary_user = para['user_vocab']\n",
    "vocabulary_item = para['item_vocab']\n",
    "train_length = para['train_length']\n",
    "test_length = para['test_length']\n",
    "u_text = para['u_text']\n",
    "i_text = para['i_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x0000023A5F7D16D8>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x0000023A5F7D14E0>\n",
      "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x0000023A5F7D1470>\n",
      "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x0000023A5F7D1240>\n",
      "F=<absl.flags._flag.Flag object at 0x0000023A5F7D10B8>\n",
      "FILTER_SIZES=<absl.flags._flag.Flag object at 0x0000023A5F7D1208>\n",
      "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x0000023A5F7D13C8>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x0000023A5F7D1780>\n",
      "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x0000023A5F7D1550>\n",
      "NUM_FILTERS=<absl.flags._flag.Flag object at 0x0000023A5F7D12B0>\n",
      "PARA_DATA=<absl.flags._flag.Flag object at 0x0000023A5F7D1080>\n",
      "TRAIN_DATA=<absl.flags._flag.Flag object at 0x0000023A5F7D1048>\n",
      "VALID_DATA=<absl.flags._flag.Flag object at 0x0000023A58DEDCF8>\n",
      "WORD2VEC=<absl.flags._flag.Flag object at 0x0000023A59143470>\n",
      "\n",
      "Loading data...\n",
      "num users: 1429\n",
      "num items: 900\n",
      "9\n",
      "198\n",
      "16\n",
      "198\n",
      "Tensor(\"attention/transpose_1:0\", shape=(?, 9, 1), dtype=float32)\n",
      "1429\n",
      "900\n",
      "Load glove u file ../../glove.6B.50d.txt\n",
      "\n",
      "Load glove u file ../../glove.6B.50d.txt\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 14963 and 14670. Shapes are [14963,50] and [14670,50]. for 'Assign_1' (op: 'Assign') with input shapes: [14963,50], [14670,50].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 14963 and 14670. Shapes are [14963,50] and [14670,50]. for 'Assign_1' (op: 'Assign') with input shapes: [14963,50], [14670,50].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-04f29079ecb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0minitW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_glove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglove_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1760\u001b[0m     \"\"\"\n\u001b[0;32m   1761\u001b[0m     assign = state_ops.assign(self._variable, value, use_locking=use_locking,\n\u001b[1;32m-> 1762\u001b[1;33m                               name=name)\n\u001b[0m\u001b[0;32m   1763\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m    221\u001b[0m     return gen_state_ops.assign(\n\u001b[0;32m    222\u001b[0m         \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    224\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m     61\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m     62\u001b[0m         \u001b[1;34m\"Assign\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                   use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m     64\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 14963 and 14670. Shapes are [14963,50] and [14670,50]. for 'Assign_1' (op: 'Assign') with input shapes: [14963,50], [14670,50]."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    FLAGS = tf.flags.FLAGS\n",
    "    FLAGS(sys.argv)\n",
    "    print(\"\\nParameters:\")\n",
    "    for attr, value in sorted(FLAGS.__flags.items()):\n",
    "        print(\"{}={}\".format(attr.upper(), value))\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    pkl_file = open(FLAGS.para_data, 'rb')\n",
    "\n",
    "    para = pickle.load(pkl_file)\n",
    "    user_num = para['user_num']\n",
    "    item_num = para['item_num']\n",
    "    review_num_u = para['review_num_u']\n",
    "    review_num_i = para['review_num_i']\n",
    "    review_len_u = para['review_len_u']\n",
    "    review_len_i = para['review_len_i']\n",
    "    vocabulary_user = para['user_vocab']\n",
    "    vocabulary_item = para['item_vocab']\n",
    "    train_length = para['train_length']\n",
    "    test_length = para['test_length']\n",
    "    u_text = para['u_text']\n",
    "    i_text = para['i_text']\n",
    "\n",
    "    np.random.seed(2017)\n",
    "    random_seed = 2017\n",
    "    print(\"num users: \" + str(user_num))\n",
    "    print(\"num items: \" + str(item_num))\n",
    "    print(review_num_u)\n",
    "    print(review_len_u)\n",
    "    print(review_num_i)\n",
    "    print(review_len_i)\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement)\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            deep = NARRE.NARRE(\n",
    "                review_num_u=review_num_u,\n",
    "                review_num_i=review_num_i,\n",
    "                review_len_u=review_len_u,\n",
    "                review_len_i=review_len_i,\n",
    "                user_num=user_num,\n",
    "                item_num=item_num,\n",
    "                num_classes=1,\n",
    "                user_vocab_size=len(vocabulary_user),\n",
    "                item_vocab_size=len(vocabulary_item),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                embedding_id=32,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                attention_size=32,\n",
    "                n_latent=32)\n",
    "            tf.set_random_seed(random_seed)\n",
    "            print (user_num)\n",
    "            print (item_num)\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "            # optimizer = tf.train.AdagradOptimizer(learning_rate=0.01, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "            optimizer = tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss)\n",
    "            \n",
    "            train_op = optimizer  # .apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            if FLAGS.word2vec:\n",
    "                initW = extract_word2vec(vocabulary_user) # extract for user vocab\n",
    "                sess.run(deep.W1.assign(initW))\n",
    "                \n",
    "                initW = extract_word2vec(vocabulary_item) # extract for item vocab\n",
    "                sess.run(deep.W1.assign(initW))\n",
    "                # word2vec code\n",
    "                \n",
    "            elif use_glove:\n",
    "                initW = extract_glove(vocabulary_user, glove_file)\n",
    "                sess.run(deep.W1.assign(initW))\n",
    "                \n",
    "                initW = extract_glove(vocabulary_item, glove_file)\n",
    "                sess.run(deep.W1.assign(initW))\n",
    "\n",
    "            epoch = 1\n",
    "            best_mae = 5\n",
    "            best_rmse = 5\n",
    "            train_mae = 0\n",
    "            train_rmse = 0\n",
    "\n",
    "            pkl_file = open(FLAGS.train_data, 'rb')\n",
    "\n",
    "            train_data = pickle.load(pkl_file)\n",
    "\n",
    "            train_data = np.array(train_data)\n",
    "            pkl_file.close()\n",
    "\n",
    "            pkl_file = open(FLAGS.valid_data, 'rb')\n",
    "\n",
    "            test_data = pickle.load(pkl_file)\n",
    "            test_data = np.array(test_data)\n",
    "            pkl_file.close()\n",
    "\n",
    "            data_size_train = len(train_data)\n",
    "            data_size_test = len(test_data)\n",
    "            batch_size = FLAGS.batch_size\n",
    "            ll = int(len(train_data) / batch_size)\n",
    "            for epoch in range(num_epochs):\n",
    "                # Shuffle the data at each epoch\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size_train))\n",
    "                shuffled_data = train_data[shuffle_indices]\n",
    "                for batch_num in range(ll):\n",
    "\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_train)\n",
    "                    data_train = shuffled_data[start_index:end_index]\n",
    "\n",
    "                    uid, iid, reuid, reiid, y_batch = zip(*data_train)\n",
    "                    u_batch = []\n",
    "                    i_batch = []\n",
    "                    for i in range(len(uid)):\n",
    "                        u_batch.append(u_text[uid[i][0]])\n",
    "                        i_batch.append(i_text[iid[i][0]])\n",
    "                    u_batch = np.array(u_batch)\n",
    "                    i_batch = np.array(i_batch)\n",
    "\n",
    "                    t_rmse, t_mae, u_a, i_a, fm = train_step(u_batch, i_batch, uid, iid, reuid, reiid, y_batch,batch_num)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    train_rmse += t_rmse\n",
    "                    train_mae += t_mae\n",
    "                    if batch_num % 500 == 0 and batch_num > 1:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        print (batch_num)\n",
    "\n",
    "                        loss_s = 0\n",
    "                        accuracy_s = 0\n",
    "                        mae_s = 0\n",
    "\n",
    "                        ll_test = int(len(test_data) / batch_size) + 1\n",
    "                        for batch_num in range(ll_test):\n",
    "                            start_index = batch_num * batch_size\n",
    "                            end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                            data_test = test_data[start_index:end_index]\n",
    "\n",
    "                            userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "                            u_valid = []\n",
    "                            i_valid = []\n",
    "                            for i in range(len(userid_valid)):\n",
    "                                u_valid.append(u_text[userid_valid[i][0]])\n",
    "                                i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                            u_valid = np.array(u_valid)\n",
    "                            i_valid = np.array(i_valid)\n",
    "\n",
    "                            loss, accuracy, mae = dev_step(u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid,\n",
    "                                                           y_valid)\n",
    "                            loss_s = loss_s + len(u_valid) * loss\n",
    "                            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                            mae_s = mae_s + len(u_valid) * mae\n",
    "                        print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                         np.sqrt(\n",
    "                                                                                             accuracy_s / test_length),\n",
    "                                                                                         mae_s / test_length))\n",
    "                        rmse = np.sqrt(accuracy_s / test_length)\n",
    "                        mae = mae_s / test_length\n",
    "                        if best_rmse > rmse:\n",
    "                            best_rmse = rmse\n",
    "                        if best_mae > mae:\n",
    "                            best_mae = mae\n",
    "                        print(\"\")\n",
    "\n",
    "                print(str(epoch) + ':\\n')\n",
    "                print(\"\\nEvaluation:\")\n",
    "                print (\"train:rmse,mae:\", train_rmse / ll, train_mae / ll)\n",
    "                u_a = np.reshape(u_a[0], (1, -1))\n",
    "                i_a = np.reshape(i_a[0], (1, -1))\n",
    "\n",
    "                print (u_a)\n",
    "                print (i_a)\n",
    "                train_rmse = 0\n",
    "                train_mae = 0\n",
    "\n",
    "                loss_s = 0\n",
    "                accuracy_s = 0\n",
    "                mae_s = 0\n",
    "\n",
    "                ll_test = int(len(test_data) / batch_size) + 1\n",
    "                for batch_num in range(ll_test):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                    data_test = test_data[start_index:end_index]\n",
    "\n",
    "                    userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "                    u_valid = []\n",
    "                    i_valid = []\n",
    "                    for i in range(len(userid_valid)):\n",
    "                        u_valid.append(u_text[userid_valid[i][0]])\n",
    "                        i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                    u_valid = np.array(u_valid)\n",
    "                    i_valid = np.array(i_valid)\n",
    "\n",
    "                    loss, accuracy, mae = dev_step(u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid, y_valid)\n",
    "                    loss_s = loss_s + len(u_valid) * loss\n",
    "                    accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                    mae_s = mae_s + len(u_valid) * mae\n",
    "                print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                 np.sqrt(accuracy_s / test_length),\n",
    "                                                                                 mae_s / test_length))\n",
    "                rmse = np.sqrt(accuracy_s / test_length)\n",
    "                mae = mae_s / test_length\n",
    "                if best_rmse > rmse:\n",
    "                    best_rmse = rmse\n",
    "                if best_mae > mae:\n",
    "                    best_mae = mae\n",
    "                print(\"\")\n",
    "            print ('best rmse:', best_rmse)\n",
    "            print ('best mae:', best_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
