{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified code from: https://github.com/chenchongthu/NARRE/blob/master/pro_data/data_pro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "tf.flags.DEFINE_string(\"valid_data\", \"../data/music_valid.csv\", \" Data for validation\")\n",
    "tf.flags.DEFINE_string(\"test_data\", \"../data/music_test.csv\", \"Data for testing\")\n",
    "tf.flags.DEFINE_string(\"train_data\", \"../data/music_train.csv\", \"Data for training\")\n",
    "tf.flags.DEFINE_string(\"user_review\", \"../data/user_review\", \"User's reviews\")\n",
    "tf.flags.DEFINE_string(\"item_review\", \"../data/item_review\", \"Item's reviews\")\n",
    "tf.flags.DEFINE_string(\"user_review_id\", \"../data/user_rid\", \"user_review_id\")\n",
    "tf.flags.DEFINE_string(\"item_review_id\", \"../data/item_rid\", \"item_review_id\")\n",
    "tf.flags.DEFINE_string(\"stopwords\", \"../data/stopwords\", \"stopwords\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def pad_sentences(u_text, u_len, u2_len, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    review_num = u_len\n",
    "    review_len = u2_len\n",
    "\n",
    "    u_text2 = {}\n",
    "    for i in u_text.keys():\n",
    "        u_reviews = u_text[i]\n",
    "        padded_u_train = []\n",
    "        for ri in range(review_num):\n",
    "            if ri < len(u_reviews):\n",
    "                sentence = u_reviews[ri]\n",
    "                if review_len > len(sentence):\n",
    "                    num_padding = review_len - len(sentence)\n",
    "                    new_sentence = sentence + [padding_word] * num_padding\n",
    "                    padded_u_train.append(new_sentence)\n",
    "                else:\n",
    "                    new_sentence = sentence[:review_len]\n",
    "                    padded_u_train.append(new_sentence)\n",
    "            else:\n",
    "                new_sentence = [padding_word] * review_len\n",
    "                padded_u_train.append(new_sentence)\n",
    "        u_text2[i] = padded_u_train\n",
    "\n",
    "    return u_text2\n",
    "\n",
    "\n",
    "def pad_reviewid(u_train, u_valid, u_len, num):\n",
    "    pad_u_train = []\n",
    "\n",
    "    for i in range(len(u_train)):\n",
    "        x = u_train[i]\n",
    "        while u_len > len(x):\n",
    "            x.append(num)\n",
    "        if u_len < len(x):\n",
    "            x = x[:u_len]\n",
    "        pad_u_train.append(x)\n",
    "    pad_u_valid = []\n",
    "\n",
    "    for i in range(len(u_valid)):\n",
    "        x = u_valid[i]\n",
    "        while u_len > len(x):\n",
    "            x.append(num)\n",
    "        if u_len < len(x):\n",
    "            x = x[:u_len]\n",
    "        pad_u_valid.append(x)\n",
    "    return pad_u_train, pad_u_valid\n",
    "\n",
    "\n",
    "def build_vocab(sentences1, sentences2):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts1 = Counter(itertools.chain(*sentences1))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv1 = [x[0] for x in word_counts1.most_common()]\n",
    "    vocabulary_inv1 = list(sorted(vocabulary_inv1))\n",
    "    # Mapping from word to index\n",
    "    vocabulary1 = {x: i for i, x in enumerate(vocabulary_inv1)}\n",
    "\n",
    "    word_counts2 = Counter(itertools.chain(*sentences2))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv2 = [x[0] for x in word_counts2.most_common()]\n",
    "    vocabulary_inv2 = list(sorted(vocabulary_inv2))\n",
    "    # Mapping from word to index\n",
    "    vocabulary2 = {x: i for i, x in enumerate(vocabulary_inv2)}\n",
    "    return [vocabulary1, vocabulary_inv1, vocabulary2, vocabulary_inv2]\n",
    "\n",
    "\n",
    "def build_input_data(u_text, i_text, vocabulary_u, vocabulary_i):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    l = len(u_text)\n",
    "    u_text2 = {}\n",
    "    for i in u_text.keys():\n",
    "        u_reviews = u_text[i]\n",
    "        u = np.array([[vocabulary_u[word] for word in words] for words in u_reviews])\n",
    "        u_text2[i] = u\n",
    "    l = len(i_text)\n",
    "    i_text2 = {}\n",
    "    for j in i_text.keys():\n",
    "        i_reviews = i_text[j]\n",
    "        i = np.array([[vocabulary_i[word] for word in words] for words in i_reviews])\n",
    "        i_text2[j] = i\n",
    "    return u_text2, i_text2\n",
    "\n",
    "\n",
    "def load_data(train_data, valid_data, user_review, item_review, user_rid, item_rid, stopwords):\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    u_text, i_text, y_train, y_valid, u_len, i_len, u2_len, i2_len, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num \\\n",
    "        , reid_user_train, reid_item_train, reid_user_valid, reid_item_valid, item_text = \\\n",
    "        load_data_and_labels(train_data, valid_data, user_review, item_review, user_rid, item_rid, stopwords)\n",
    "    print (\"load data done\")\n",
    "    u_text = pad_sentences(u_text, u_len, u2_len)\n",
    "    reid_user_train, reid_user_valid = pad_reviewid(reid_user_train, reid_user_valid, u_len, item_num + 1)\n",
    "\n",
    "    print (\"pad user done\")\n",
    "    i_text = pad_sentences(i_text, i_len, i2_len)\n",
    "    reid_item_train, reid_item_valid = pad_reviewid(reid_item_train, reid_item_valid, i_len, user_num + 1)\n",
    "    print (\"pad item done\")\n",
    "\n",
    "    user_voc = [xx for x in u_text.values() for xx in x]\n",
    "    item_voc = [xx for x in i_text.values() for xx in x]\n",
    "    \n",
    "    vocabulary_user, vocabulary_inv_user, vocabulary_item, vocabulary_inv_item = build_vocab(user_voc, item_voc)\n",
    "    print (len(vocabulary_user))\n",
    "    print (len(vocabulary_item))\n",
    "    u_text, i_text = build_input_data(u_text, i_text, vocabulary_user, vocabulary_item)\n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    uid_train = np.array(uid_train)\n",
    "    uid_valid = np.array(uid_valid)\n",
    "    iid_train = np.array(iid_train)\n",
    "    iid_valid = np.array(iid_valid)\n",
    "    reid_user_train = np.array(reid_user_train)\n",
    "    reid_user_valid = np.array(reid_user_valid)\n",
    "    reid_item_train = np.array(reid_item_train)\n",
    "    reid_item_valid = np.array(reid_item_valid)\n",
    "\n",
    "    return [u_text, i_text, y_train, y_valid, vocabulary_user, vocabulary_inv_user, vocabulary_item,\n",
    "            vocabulary_inv_item, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num, reid_user_train,\n",
    "            reid_item_train, reid_user_valid, reid_item_valid, item_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_data, valid_data, user_review, item_review, user_rid, item_rid, stopwords):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    f_train = open(train_data, \"r\")\n",
    "    f1 = open(user_review, \"rb\")\n",
    "    f2 = open(item_review, \"rb\")\n",
    "    f3 = open(user_rid, \"rb\")\n",
    "    f4 = open(item_rid, \"rb\")\n",
    "\n",
    "    user_reviews = pickle.load(f1)\n",
    "    item_reviews = pickle.load(f2)\n",
    "    user_rids = pickle.load(f3)\n",
    "    item_rids = pickle.load(f4)\n",
    "\n",
    "    reid_user_train = []\n",
    "    reid_item_train = []\n",
    "    uid_train = []\n",
    "    iid_train = []\n",
    "    y_train = []\n",
    "    u_text = {}\n",
    "    u_rid = {}\n",
    "    i_text = {}\n",
    "    i_rid = {}\n",
    "    i = 0\n",
    "    for line in f_train:\n",
    "        i = i + 1\n",
    "        line = line.split(',')\n",
    "        uid_train.append(int(line[0]))\n",
    "        iid_train.append(int(line[1]))\n",
    "        if int(line[0]) in u_text:\n",
    "        #if u_text.has_key(int(line[0])):\n",
    "            reid_user_train.append(u_rid[int(line[0])])\n",
    "        else:\n",
    "            u_text[int(line[0])] = []\n",
    "            for s in user_reviews[int(line[0])]:\n",
    "                s1 = clean_str(s)\n",
    "                s1 = s1.split(\" \")\n",
    "                u_text[int(line[0])].append(s1)\n",
    "            u_rid[int(line[0])] = []\n",
    "            for s in user_rids[int(line[0])]:\n",
    "                u_rid[int(line[0])].append(int(s))\n",
    "            reid_user_train.append(u_rid[int(line[0])])\n",
    "\n",
    "        if int(line[1]) in i_text:\n",
    "        #if i_text.has_key(int(line[1])):\n",
    "            reid_item_train.append(i_rid[int(line[1])])  #####write here\n",
    "        else:\n",
    "            i_text[int(line[1])] = []\n",
    "            for s in item_reviews[int(line[1])]:\n",
    "                s1 = clean_str(s)\n",
    "                s1 = s1.split(\" \")\n",
    "\n",
    "                i_text[int(line[1])].append(s1)\n",
    "            i_rid[int(line[1])] = []\n",
    "            for s in item_rids[int(line[1])]:\n",
    "                i_rid[int(line[1])].append(int(s))\n",
    "            reid_item_train.append(i_rid[int(line[1])])\n",
    "        y_train.append(float(line[2]))\n",
    "    print (\"valid\")\n",
    "    reid_user_valid = []\n",
    "    reid_item_valid = []\n",
    "\n",
    "    uid_valid = []\n",
    "    iid_valid = []\n",
    "    y_valid = []\n",
    "    f_valid = open(valid_data)\n",
    "    for line in f_valid:\n",
    "        line = line.split(',')\n",
    "        uid_valid.append(int(line[0]))\n",
    "        iid_valid.append(int(line[1]))\n",
    "        if int(line[0]) in u_text:\n",
    "        #if u_text.has_key(int(line[0])):\n",
    "            reid_user_valid.append(u_rid[int(line[0])])\n",
    "        else:\n",
    "            u_text[int(line[0])] = [['<PAD/>']]\n",
    "            u_rid[int(line[0])] = [int(0)]\n",
    "            reid_user_valid.append(u_rid[int(line[0])])\n",
    "\n",
    "        if int(line[1]) in i_text:\n",
    "        #if i_text.has_key(int(line[1])):\n",
    "            reid_item_valid.append(i_rid[int(line[1])])\n",
    "        else:\n",
    "            i_text[int(line[1])] = [['<PAD/>']]\n",
    "            i_rid[int(line[1])] = [int(0)]\n",
    "            reid_item_valid.append(i_rid[int(line[1])])\n",
    "\n",
    "        y_valid.append(float(line[2]))\n",
    "    print (\"len\")\n",
    "\n",
    "\n",
    "    review_num_u = np.array([len(x) for x in u_text.values()])\n",
    "    x = np.sort(review_num_u)\n",
    "    u_len = x[int(0.9 * len(review_num_u)) - 1]\n",
    "    review_len_u = np.array([len(j) for i in u_text.values() for j in i])\n",
    "    x2 = np.sort(review_len_u)\n",
    "    u2_len = x2[int(0.9 * len(review_len_u)) - 1]\n",
    "\n",
    "    review_num_i = np.array([len(x) for x in i_text.values()])\n",
    "    y = np.sort(review_num_i)\n",
    "    i_len = y[int(0.9 * len(review_num_i)) - 1]\n",
    "    review_len_i = np.array([len(j) for i in i_text.values() for j in i])\n",
    "    y2 = np.sort(review_len_i)\n",
    "    i2_len = y2[int(0.9 * len(review_len_i)) - 1]\n",
    "    print (\"u_len:\", u_len)\n",
    "    print (\"i_len:\", i_len)\n",
    "    print (\"u2_len:\", u2_len)\n",
    "    print (\"i2_len:\", i2_len)\n",
    "    user_num = len(u_text)\n",
    "    item_num = len(i_text)\n",
    "    print (\"user_num:\", user_num)\n",
    "    print (\"item_num:\", item_num)\n",
    "    item_text = i_text\n",
    "\n",
    "    return [u_text, i_text, y_train, y_valid, u_len, i_len, u2_len, i2_len, uid_train,\n",
    "            iid_train, uid_valid, iid_valid, user_num,\n",
    "            item_num, reid_user_train, reid_item_train, reid_user_valid, reid_item_valid, item_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "len\n",
      "u_len: 9\n",
      "i_len: 16\n",
      "u2_len: 198\n",
      "i2_len: 198\n",
      "user_num: 1429\n",
      "item_num: 900\n",
      "load data done\n",
      "pad user done\n",
      "pad item done\n",
      "14963\n",
      "14670\n"
     ]
    }
   ],
   "source": [
    "TPS_DIR = '../data'\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "u_text, i_text, y_train, y_valid, vocabulary_user, vocabulary_inv_user, vocabulary_item, \\\n",
    "vocabulary_inv_item, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num, reid_user_train, reid_item_train, reid_user_valid, reid_item_valid, item_text= \\\n",
    "    load_data(FLAGS.train_data, FLAGS.valid_data, FLAGS.user_review, FLAGS.item_review, FLAGS.user_review_id, \\\n",
    "              FLAGS.item_review_id, FLAGS.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write begin\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2017)\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "\n",
    "userid_train = uid_train[shuffle_indices]\n",
    "itemid_train = iid_train[shuffle_indices]\n",
    "y_train = y_train[shuffle_indices]\n",
    "reid_user_train = reid_user_train[shuffle_indices]\n",
    "reid_item_train = reid_item_train[shuffle_indices]\n",
    "\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_valid = y_valid[:, np.newaxis]\n",
    "\n",
    "userid_train = userid_train[:, np.newaxis]\n",
    "itemid_train = itemid_train[:, np.newaxis]\n",
    "userid_valid = uid_valid[:, np.newaxis]\n",
    "itemid_valid = iid_valid[:, np.newaxis]\n",
    "\n",
    "batches_train = list(\n",
    "    zip(userid_train, itemid_train, reid_user_train, reid_item_train, y_train))\n",
    "batches_test = list(zip(userid_valid, itemid_valid, reid_user_valid, reid_item_valid, y_valid))\n",
    "print ('write begin')\n",
    "output = open(os.path.join(TPS_DIR, 'music.train'), 'wb')\n",
    "pickle.dump(batches_train, output)\n",
    "output = open(os.path.join(TPS_DIR, 'music.test'), 'wb')\n",
    "pickle.dump(batches_test, output)\n",
    "\n",
    "para = {}\n",
    "para['user_num'] = user_num\n",
    "para['item_num'] = item_num\n",
    "para['review_num_u'] = u_text[0].shape[0]\n",
    "para['review_num_i'] = i_text[0].shape[0]\n",
    "para['review_len_u'] = u_text[1].shape[1]\n",
    "para['review_len_i'] = i_text[1].shape[1]\n",
    "para['user_vocab'] = vocabulary_user\n",
    "para['item_vocab'] = vocabulary_item\n",
    "para['train_length'] = len(y_train)\n",
    "para['test_length'] = len(y_valid)\n",
    "para['u_text'] = u_text\n",
    "para['i_text'] = i_text\n",
    "para['item_text'] = item_text\n",
    "output = open(os.path.join(TPS_DIR, 'music.para'), 'wb')\n",
    "\n",
    "# Pickle dictionary using protocol 0.\n",
    "pickle.dump(para, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
