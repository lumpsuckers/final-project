{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string(\"valid_data\",\"../data/music_valid.csv\", \" Data for validation\")\n",
    "tf.flags.DEFINE_string(\"test_data\", \"../data/music_test.csv\", \"Data for testing\")\n",
    "tf.flags.DEFINE_string(\"train_data\", \"../data/music_train.csv\", \"Data for training\")\n",
    "tf.flags.DEFINE_string(\"user_review\", \"../data/user_review\", \"User's reviews\")\n",
    "tf.flags.DEFINE_string(\"item_review\", \"../data/item_review\", \"Item's reviews\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def pad_sentences(u_text,u_len,padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length=u_len\n",
    "    u_text2={}\n",
    "    print (len(u_text))\n",
    "    for i in u_text.keys():\n",
    "        #print i\n",
    "        sentence = u_text[i]\n",
    "        if sequence_length>len(sentence):\n",
    "\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            u_text2[i]=new_sentence\n",
    "        else:\n",
    "            new_sentence = sentence[:sequence_length]\n",
    "            u_text2[i] = new_sentence\n",
    "\n",
    "    return u_text2\n",
    "def build_vocab(sentences1,sentences2):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts1 = Counter(itertools.chain(*sentences1))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv1 = [x[0] for x in word_counts1.most_common()]\n",
    "    vocabulary_inv1 = list(sorted(vocabulary_inv1))\n",
    "    # Mapping from word to index\n",
    "    vocabulary1 = {x: i for i, x in enumerate(vocabulary_inv1)}\n",
    "\n",
    "    word_counts2 = Counter(itertools.chain(*sentences2))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv2 = [x[0] for x in word_counts2.most_common()]\n",
    "    vocabulary_inv2 = list(sorted(vocabulary_inv2))\n",
    "    # Mapping from word to index\n",
    "    vocabulary2 = {x: i for i, x in enumerate(vocabulary_inv2)}\n",
    "    return [vocabulary1, vocabulary_inv1,vocabulary2, vocabulary_inv2]\n",
    "\n",
    "def build_input_data(u_text,i_text, vocabulary_u,vocabulary_i):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    l = len(u_text)\n",
    "    u_text2 = {}\n",
    "    for i in u_text.keys():\n",
    "        u_reviews = u_text[i]\n",
    "        u = np.array([vocabulary_u[word] for word  in u_reviews])\n",
    "        u_text2[i] = u\n",
    "    l = len(i_text)\n",
    "    i_text2 = {}\n",
    "    for j in i_text.keys():\n",
    "        i_reviews = i_text[j]\n",
    "        i = np.array([vocabulary_i[word] for word in i_reviews])\n",
    "        i_text2[j] = i\n",
    "    return u_text2, i_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data,valid_data,user_review,item_review):\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    u_text,i_text, y_train, y_valid,u_len,i_len,uid_train,iid_train,uid_valid,iid_valid,user_num,item_num=\\\n",
    "        load_data_and_labels(train_data,valid_data,user_review,item_review)\n",
    "    print (\"load data done\")\n",
    "    u_text = pad_sentences(u_text,u_len)\n",
    "    print (\"pad user done\")\n",
    "    i_text=pad_sentences(i_text,i_len)\n",
    "    print (\"pad item done\")\n",
    "\n",
    "    user_voc = [x for x in u_text.values()]\n",
    "    item_voc = [x for x in i_text.values()]\n",
    "\n",
    "    vocabulary_user, vocabulary_inv_user, vocabulary_item, vocabulary_inv_item = build_vocab(user_voc, item_voc)\n",
    "    print (len(vocabulary_user))\n",
    "    print (len(vocabulary_item))\n",
    "    u_text, i_text = build_input_data(u_text, i_text, vocabulary_user, vocabulary_item)\n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    uid_train = np.array(uid_train)\n",
    "    uid_valid = np.array(uid_valid)\n",
    "    iid_train = np.array(iid_train)\n",
    "    iid_valid = np.array(iid_valid)\n",
    "\n",
    "    return [u_text,i_text,y_train,y_valid, vocabulary_user, vocabulary_inv_user,vocabulary_item,vocabulary_inv_item,\n",
    "            uid_train, iid_train, uid_valid, iid_valid, user_num, item_num]\n",
    "\n",
    "\n",
    "def load_data_and_labels(train_data,valid_data,user_review,item_review):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    f_train = open(train_data, \"r\")\n",
    "    f1 = open(user_review, \"rb\")\n",
    "    f2 = open(item_review, \"rb\")\n",
    "\n",
    "    user_reviews=pickle.load(f1)\n",
    "    item_reviews=pickle.load(f2)\n",
    "    uid_train = []\n",
    "    iid_train = []\n",
    "    y_train=[]\n",
    "    u_text = {}\n",
    "    i_text = {}\n",
    "    i = 0\n",
    "    for line in f_train:\n",
    "        i = i + 1\n",
    "        line = line.split(',')\n",
    "        uid_train.append(int(line[0]))\n",
    "        iid_train.append(int(line[1]))\n",
    "        if int(line[0]) in u_text:\n",
    "            a=1\n",
    "        else:\n",
    "            u_text[int(line[0])] = '<PAD/>'\n",
    "            for s in user_reviews[int(line[0])]:\n",
    "                u_text[int(line[0])] = u_text[int(line[0])] + ' ' + s.strip()\n",
    "            u_text[int(line[0])]=clean_str(u_text[int(line[0])])\n",
    "            u_text[int(line[0])]=u_text[int(line[0])].split(\" \")\n",
    "\n",
    "        if int(line[1]) in i_text:\n",
    "            a=1\n",
    "        else:\n",
    "            i_text[int(line[1])] = '<PAD/>'\n",
    "            for s in item_reviews[int(line[1])]:\n",
    "                i_text[int(line[1])] = i_text[int(line[1])] + ' ' + s.strip()\n",
    "            i_text[int(line[1])]=clean_str(i_text[int(line[1])])\n",
    "            i_text[int(line[1])]=i_text[int(line[1])].split(\" \")\n",
    "        y_train.append(float(line[2]))\n",
    "    print (\"valid\")\n",
    "\n",
    "    uid_valid = []\n",
    "    iid_valid = []\n",
    "    y_valid=[]\n",
    "    f_valid=open(valid_data)\n",
    "    for line in f_valid:\n",
    "        line=line.split(',')\n",
    "        uid_valid.append(int(line[0]))\n",
    "        iid_valid.append(int(line[1]))\n",
    "        if int(line[0]) in u_text:\n",
    "            a=1\n",
    "        else:\n",
    "            u_text[int(line[0])] = '<PAD/>'\n",
    "            u_text[int(line[0])]=clean_str(u_text[int(line[0])])\n",
    "            u_text[int(line[0])]=u_text[int(line[0])].split(\" \")\n",
    "\n",
    "\n",
    "        if int(line[1]) in i_text:\n",
    "            a=1\n",
    "        else:\n",
    "            i_text[int(line[1])] = '<PAD/>'\n",
    "            i_text[int(line[1])]=clean_str(i_text[int(line[1])])\n",
    "            i_text[int(line[1])]=i_text[int(line[1])].split(\" \")\n",
    "\n",
    "        y_valid.append(float(line[2]))\n",
    "    print (\"len\")\n",
    "    u = np.array([len(x) for x in u_text.values()])\n",
    "    x = np.sort(u)\n",
    "    u_len = x[int(0.85* len(u)) - 1]\n",
    "\n",
    "\n",
    "    i = np.array([len(x) for x in i_text.values()])\n",
    "    y = np.sort(i)\n",
    "    i_len = y[int(0.85 * len(i)) - 1]\n",
    "    print (\"u_len:\",u_len)\n",
    "    print (\"i_len:\",i_len)\n",
    "    user_num = len(u_text)\n",
    "    item_num = len(i_text)\n",
    "    print (\"user_num:\", user_num)\n",
    "    print (\"item_num:\", item_num)\n",
    "    return [u_text,i_text,y_train,y_valid,u_len,i_len,uid_train,iid_train,uid_valid,iid_valid,user_num,item_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "len\n",
      "u_len: 834\n",
      "i_len: 1390\n",
      "user_num: 1429\n",
      "item_num: 900\n",
      "load data done\n",
      "1429\n",
      "pad user done\n",
      "900\n",
      "pad item done\n",
      "14984\n",
      "15822\n"
     ]
    }
   ],
   "source": [
    "TPS_DIR = '../data'\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS(sys.argv)\n",
    "#FLAGS._parse_flags()\n",
    "\n",
    "u_text,i_text, y_train, y_valid, vocabulary_user, vocabulary_inv_user, vocabulary_item, \\\n",
    "vocabulary_inv_item, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num = \\\n",
    "    load_data(FLAGS.train_data, FLAGS.valid_data, FLAGS.user_review, FLAGS.item_review)\n",
    "\n",
    "np.random.seed(2017)\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "\n",
    "userid_train = uid_train[shuffle_indices]\n",
    "itemid_train = iid_train[shuffle_indices]\n",
    "y_train = y_train[shuffle_indices]\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_valid = y_valid[:, np.newaxis]\n",
    "\n",
    "userid_train = userid_train[:, np.newaxis]\n",
    "itemid_train = itemid_train[:, np.newaxis]\n",
    "userid_valid = uid_valid[:, np.newaxis]\n",
    "itemid_valid = iid_valid[:, np.newaxis]\n",
    "\n",
    "batches_train=list(zip( userid_train, itemid_train, y_train))\n",
    "batches_test=list(zip(userid_valid,itemid_valid,y_valid))\n",
    "output = open(os.path.join(TPS_DIR, 'music.train'), 'wb')\n",
    "pickle.dump(batches_train,output)\n",
    "output = open(os.path.join(TPS_DIR, 'music.test'), 'wb')\n",
    "pickle.dump(batches_test,output)\n",
    "\n",
    "para={}\n",
    "para['user_num']=user_num\n",
    "para['item_num']=item_num\n",
    "para['user_length']=u_text[0].shape[0]\n",
    "para['item_length'] = i_text[0].shape[0]\n",
    "para['user_vocab'] = vocabulary_user\n",
    "para['item_vocab'] = vocabulary_item\n",
    "para['train_length']=len(y_train)\n",
    "para['test_length']=len(y_valid)\n",
    "para['u_text'] = u_text\n",
    "para['i_text'] = i_text\n",
    "output = open(os.path.join(TPS_DIR, 'music.para'), 'wb')\n",
    "\n",
    "pickle.dump(para, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
