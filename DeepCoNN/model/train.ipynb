{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.contrib import learn\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "import DeepCoNN\n",
    "import sys\n",
    "\n",
    "\n",
    "tf.flags.DEFINE_string(\"word2vec\", None, \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "tf.flags.DEFINE_string(\"valid_data\",\"../data/music.test\", \" Data for validation\")\n",
    "tf.flags.DEFINE_string(\"para_data\", \"../data/music.para\", \"Data parameters\")\n",
    "tf.flags.DEFINE_string(\"train_data\", \"../data/music.train\", \"Data for training\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "#tf.flags.DEFINE_string(\"word2vec\", \"./data/rt-polaritydata/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 50, \"Dimensionality of character embedding \")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3\", \"Comma-separated filter sizes \")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability \")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.001, \"L2 regularizaion lambda\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_V\", 0, \"L2 regularizaion V\")\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\",100, \"Batch Size \")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 5, \"Number of training epochs \")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps \")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps \")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(u_batch, i_batch, uid, iid, y_batch, batch_num):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "    }\n",
    "    _, step, loss, accuracy, mae = sess.run(\n",
    "        [train_op, global_step, deep.loss, deep.accuracy, deep.mae],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "    # print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, accuracy, mae))\n",
    "    return accuracy, mae\n",
    "\n",
    "\n",
    "def dev_step(u_batch, i_batch, uid, iid, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, loss, accuracy, mae = sess.run(\n",
    "        [global_step, deep.loss, deep.accuracy, deep.mae],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    # print(\"{}: step{}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, step, loss, accuracy, mae))\n",
    "\n",
    "    return [loss, accuracy, mae]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word2vec(vocabulary):\n",
    "    # initial matrix with random uniform\n",
    "    u = 0\n",
    "    initW = np.random.uniform(-1.0, 1.0, (len(vocabulary), FLAGS.embedding_dim))\n",
    "    # load any vectors from the word2vec\n",
    "    print(\"Load word2vec u file {}\\n\".format(FLAGS.word2vec))\n",
    "    with open(FLAGS.word2vec, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            idx = 0\n",
    "\n",
    "            if word in vocabulary:\n",
    "                u = u + 1\n",
    "                idx = vocabulary[word]\n",
    "                initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return initW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_glove(vocabulary, glove_file, vocab_size=400000):\n",
    "    \"\"\"\n",
    "    Use pre-trained GloVe embedding as alternative to word2vec.\n",
    "    Load in file with specified vocab size and dimensional embedding size.\n",
    "    If word exists in user/item vocabulary, replace the row with the values.\n",
    "    \"\"\"\n",
    "    # initial matrix with random uniform\n",
    "    u = 0\n",
    "    initW = np.random.uniform(-1.0, 1.0, (len(vocabulary), FLAGS.embedding_dim))\n",
    "    \n",
    "    print(\"Load glove u file {}\\n\".format(glove_file))\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i in range(vocab_size):\n",
    "            line = f.readline()\n",
    "            word_embed = line.split()\n",
    "            word = word_embed[0]\n",
    "            embed = [float(x) for x in word_embed[1:]]\n",
    "            idx = 0\n",
    "\n",
    "            if word in vocabulary:\n",
    "                u = u + 1\n",
    "                idx = vocabulary[word]\n",
    "                initW[idx] = embed\n",
    "    return initW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = \"../data/glove.6B.50d.txt\" \n",
    "use_glove = True # turn on if use glove instead of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x0000011D05FBD240>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x0000011D05FB5E80>\n",
      "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x0000011D05FBD0B8>\n",
      "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x0000011D05FB5DA0>\n",
      "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x0000011D05FB5B70>\n",
      "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x0000011D05FBD0F0>\n",
      "F=<absl.flags._flag.Flag object at 0x0000011D05FB59E8>\n",
      "FILTER_SIZES=<absl.flags._flag.Flag object at 0x0000011D05FB5B38>\n",
      "L2_REG_V=<absl.flags._flag.Flag object at 0x0000011D05FB5E10>\n",
      "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x0000011D05FB5CF8>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x0000011D05FBD2E8>\n",
      "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x0000011D05FBD048>\n",
      "NUM_FILTERS=<absl.flags._flag.Flag object at 0x0000011D05FB5BE0>\n",
      "PARA_DATA=<absl.flags._flag.Flag object at 0x0000011D05FB59B0>\n",
      "TRAIN_DATA=<absl.flags._flag.Flag object at 0x0000011D05FB5978>\n",
      "VALID_DATA=<absl.flags._flag.Flag object at 0x0000011D7A38DF28>\n",
      "WORD2VEC=<absl.flags._flag.Flag object at 0x0000011D7A38DFD0>\n",
      "\n",
      "Loading data...\n",
      "Tensor(\"fm/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"fm/add_1:0\", shape=(?, 1), dtype=float32)\n",
      "83.09\n",
      "0:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 1.1022331925427042 0.8556375285474266\n",
      "loss_valid 46.2497, rmse_valid 1.0236, mae_valid 0.796916\n",
      "\n",
      "1:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.8671765661821133 0.6446797368003101\n",
      "loss_valid 45.5494, rmse_valid 1.01512, mae_valid 0.847488\n",
      "\n",
      "2:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.842065856951039 0.6219498228009154\n",
      "loss_valid 44.4234, rmse_valid 1.00492, mae_valid 0.724583\n",
      "\n",
      "3:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.8282889701971193 0.5990426722096234\n",
      "loss_valid 43.683, rmse_valid 0.996386, mae_valid 0.74884\n",
      "\n",
      "4:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.8181966586810786 0.593746716292893\n",
      "loss_valid 45.255, rmse_valid 1.01532, mae_valid 0.707861\n",
      "\n",
      "5:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.8078725912222048 0.5817985309333336\n",
      "loss_valid 44.5366, rmse_valid 1.00612, mae_valid 0.765121\n",
      "\n",
      "6:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.8012063299737325 0.5709054691762458\n",
      "loss_valid 44.5978, rmse_valid 1.0069, mae_valid 0.793831\n",
      "\n",
      "7:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7985288653431869 0.5700332943986102\n",
      "loss_valid 45.944, rmse_valid 1.02347, mae_valid 0.704988\n",
      "\n",
      "8:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7984746876286297 0.5681059880227577\n",
      "loss_valid 45.1903, rmse_valid 1.01363, mae_valid 0.790164\n",
      "\n",
      "9:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7942778388174568 0.5663223063073507\n",
      "loss_valid 45.2433, rmse_valid 1.01633, mae_valid 0.703775\n",
      "\n",
      "10:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7888351918720617 0.5567418631257081\n",
      "loss_valid 44.8634, rmse_valid 1.01108, mae_valid 0.750258\n",
      "\n",
      "11:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7931224950929967 0.5650582139084979\n",
      "loss_valid 45.3113, rmse_valid 1.01606, mae_valid 0.71082\n",
      "\n",
      "12:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7977831501786302 0.5677072881925397\n",
      "loss_valid 45.1049, rmse_valid 1.01363, mae_valid 0.74175\n",
      "\n",
      "13:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7905364414540733 0.5602293032698515\n",
      "loss_valid 46.7041, rmse_valid 1.03215, mae_valid 0.70035\n",
      "\n",
      "14:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7990082989378673 0.5705867351555243\n",
      "loss_valid 48.4965, rmse_valid 1.04887, mae_valid 0.886817\n",
      "\n",
      "15:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.78721897049648 0.5580156623590283\n",
      "loss_valid 45.1517, rmse_valid 1.01459, mae_valid 0.721239\n",
      "\n",
      "16:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7875525784201738 0.5568918742057754\n",
      "loss_valid 46.023, rmse_valid 1.02481, mae_valid 0.709052\n",
      "\n",
      "17:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7863978256539601 0.5544694513082504\n",
      "loss_valid 45.386, rmse_valid 1.01646, mae_valid 0.748147\n",
      "\n",
      "18:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7797158564009318 0.5492865105227727\n",
      "loss_valid 45.5551, rmse_valid 1.01832, mae_valid 0.773161\n",
      "\n",
      "19:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7816922817288375 0.5490948001785976\n",
      "loss_valid 45.3545, rmse_valid 1.01603, mae_valid 0.766033\n",
      "\n",
      "20:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7826511481913124 0.5535499373587166\n",
      "loss_valid 45.4443, rmse_valid 1.01687, mae_valid 0.78266\n",
      "\n",
      "21:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7891633430632149 0.5597541612095949\n",
      "loss_valid 44.7889, rmse_valid 1.00988, mae_valid 0.749397\n",
      "\n",
      "22:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.785170505686504 0.5536980596257419\n",
      "loss_valid 45.6265, rmse_valid 1.01909, mae_valid 0.767719\n",
      "\n",
      "23:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7892345379038554 0.5590824397598825\n",
      "loss_valid 45.3223, rmse_valid 1.01571, mae_valid 0.738566\n",
      "\n",
      "24:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7808078970850968 0.5497353244118575\n",
      "loss_valid 45.3933, rmse_valid 1.01659, mae_valid 0.731312\n",
      "\n",
      "25:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7819691729254838 0.5500952001751923\n",
      "loss_valid 45.5406, rmse_valid 1.01883, mae_valid 0.738892\n",
      "\n",
      "26:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7831991707406393 0.5502414812402028\n",
      "loss_valid 45.446, rmse_valid 1.01786, mae_valid 0.750945\n",
      "\n",
      "27:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7802122715042858 0.5490021451217372\n",
      "loss_valid 45.4093, rmse_valid 1.01749, mae_valid 0.733638\n",
      "\n",
      "28:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7765783289583718 0.5448884854956371\n",
      "loss_valid 45.89, rmse_valid 1.02247, mae_valid 0.726986\n",
      "\n",
      "29:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7806130349636078 0.5460336800755524\n",
      "loss_valid 49.5458, rmse_valid 1.06024, mae_valid 0.893001\n",
      "\n",
      "30:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7860295132892888 0.5566906576476446\n",
      "loss_valid 46.6089, rmse_valid 1.0313, mae_valid 0.703001\n",
      "\n",
      "31:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7825670947388905 0.5506013940747191\n",
      "loss_valid 45.884, rmse_valid 1.0228, mae_valid 0.722005\n",
      "\n",
      "32:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.779795840019133 0.5488716229432966\n",
      "loss_valid 45.4313, rmse_valid 1.01763, mae_valid 0.728652\n",
      "\n",
      "33:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7807510800477935 0.5522623908955876\n",
      "loss_valid 45.428, rmse_valid 1.01797, mae_valid 0.720829\n",
      "\n",
      "34:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7785242926783678 0.5477907359600067\n",
      "loss_valid 46.0232, rmse_valid 1.0233, mae_valid 0.788665\n",
      "\n",
      "35:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7753352769991246 0.5428535298603338\n",
      "loss_valid 45.8315, rmse_valid 1.02211, mae_valid 0.72116\n",
      "\n",
      "36:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7785914736550029 0.5462260805979008\n",
      "loss_valid 47.2056, rmse_valid 1.03848, mae_valid 0.701671\n",
      "\n",
      "37:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7741191932340947 0.5418057747003509\n",
      "loss_valid 47.2012, rmse_valid 1.0378, mae_valid 0.700668\n",
      "\n",
      "38:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.776758685344603 0.5458407998085022\n",
      "loss_valid 45.589, rmse_valid 1.01884, mae_valid 0.725482\n",
      "\n",
      "39:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.7800053816016127 0.5517602621055231\n",
      "loss_valid 45.2901, rmse_valid 1.01529, mae_valid 0.756355\n",
      "\n",
      "best rmse: 0.9963864639159685\n",
      "best mae: 0.7003501264889348\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    FLAGS = tf.flags.FLAGS\n",
    "    FLAGS(sys.argv)\n",
    "    print(\"\\nParameters:\")\n",
    "    for attr, value in sorted(FLAGS.__flags.items()):\n",
    "        print(\"{}={}\".format(attr.upper(), value))\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    pkl_file = open(FLAGS.para_data, 'rb')\n",
    "\n",
    "    para = pickle.load(pkl_file)\n",
    "    user_num = para['user_num']\n",
    "    item_num = para['item_num']\n",
    "    user_length = para['user_length']\n",
    "    item_length = para['item_length']\n",
    "    vocabulary_user = para['user_vocab']\n",
    "    vocabulary_item = para['item_vocab']\n",
    "    train_length = para['train_length']\n",
    "    test_length = para['test_length']\n",
    "    u_text = para['u_text']\n",
    "    i_text = para['i_text']\n",
    "\n",
    "    np.random.seed(2017)\n",
    "    random_seed = 2017\n",
    "    k = 100\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement)\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            deep = DeepCoNN.DeepCoNN(\n",
    "                user_num=user_num,\n",
    "                item_num=item_num,\n",
    "                user_length=user_length,\n",
    "                item_length=item_length,\n",
    "                num_classes=1,\n",
    "                user_vocab_size=len(vocabulary_user),\n",
    "                item_vocab_size=len(vocabulary_item),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                fm_k=8,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                l2_reg_V=FLAGS.l2_reg_V,\n",
    "                n_latent=32)\n",
    "            tf.set_random_seed(random_seed)\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "            # optimizer = tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss)\n",
    "            '''optimizer=tf.train.RMSPropOptimizer(0.002)\n",
    "            grads_and_vars = optimizer.compute_gradients(deep.loss)'''\n",
    "            train_op = optimizer  # .apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            \n",
    "            if FLAGS.word2vec:\n",
    "                initW = extract_word2vec(vocabulary_user) # extract for user vocab\n",
    "                sess.run(deep.W1.assign(initW))\n",
    "                \n",
    "                initW = extract_word2vec(vocabulary_item) # extract for item vocab\n",
    "                sess.run(deep.W2.assign(initW))\n",
    "                # word2vec code\n",
    "                \n",
    "            #elif use_glove:\n",
    "            #    initW = extract_glove(vocabulary_user, glove_file)\n",
    "            #    sess.run(deep.W1.assign(initW))\n",
    "             #   \n",
    "              #  initW = extract_glove(vocabulary_item, glove_file)\n",
    "              #  sess.run(deep.W2.assign(initW))\n",
    "\n",
    "\n",
    "\n",
    "            l = (train_length / FLAGS.batch_size) + 1\n",
    "            print (l)\n",
    "            ll = 0\n",
    "            epoch = 1\n",
    "            best_mae = 5\n",
    "            best_rmse = 5\n",
    "            train_mae = 0\n",
    "            train_rmse = 0\n",
    "\n",
    "            pkl_file = open(FLAGS.train_data, 'rb')\n",
    "\n",
    "            train_data = pickle.load(pkl_file)\n",
    "\n",
    "            train_data = np.array(train_data)\n",
    "            pkl_file.close()\n",
    "\n",
    "            pkl_file = open(FLAGS.valid_data, 'rb')\n",
    "\n",
    "            test_data = pickle.load(pkl_file)\n",
    "            test_data = np.array(test_data)\n",
    "            pkl_file.close()\n",
    "\n",
    "            data_size_train = len(train_data)\n",
    "            data_size_test = len(test_data)\n",
    "            batch_size = 100\n",
    "            ll = int(len(train_data) / batch_size)\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                # Shuffle the data at each epoch\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size_train))\n",
    "                shuffled_data = train_data[shuffle_indices]\n",
    "                for batch_num in range(ll):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_train)\n",
    "                    data_train = shuffled_data[start_index:end_index]\n",
    "\n",
    "                    uid, iid, y_batch = zip(*data_train)\n",
    "\n",
    "                    u_batch = []\n",
    "                    i_batch = []\n",
    "                    for i in range(len(uid)):\n",
    "                        u_batch.append(u_text[uid[i][0]])\n",
    "                        i_batch.append(i_text[iid[i][0]])\n",
    "                    u_batch = np.array(u_batch)\n",
    "                    i_batch = np.array(i_batch)\n",
    "\n",
    "                    t_rmse, t_mae = train_step(u_batch, i_batch, uid, iid, y_batch, batch_num)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    train_rmse += t_rmse\n",
    "                    train_mae += t_mae\n",
    "\n",
    "                    if batch_num % 1000 == 0 and batch_num > 1:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        print (batch_num)\n",
    "                        loss_s = 0\n",
    "                        accuracy_s = 0\n",
    "                        mae_s = 0\n",
    "\n",
    "                        ll_test = int(len(test_data) / batch_size) + 1\n",
    "                        for batch_num2 in range(ll_test):\n",
    "                            start_index = batch_num2 * batch_size\n",
    "                            end_index = min((batch_num2 + 1) * batch_size, data_size_test)\n",
    "                            data_test = test_data[start_index:end_index]\n",
    "\n",
    "                            userid_valid, itemid_valid, y_valid = zip(*data_test)\n",
    "\n",
    "                            u_valid = []\n",
    "                            i_valid = []\n",
    "                            for i in range(len(userid_valid)):\n",
    "                                u_valid.append(u_text[userid_valid[i][0]])\n",
    "                                i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                            u_valid = np.array(u_valid)\n",
    "                            i_valid = np.array(i_valid)\n",
    "\n",
    "                            loss, accuracy, mae = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid)\n",
    "                            loss_s = loss_s + len(u_valid) * loss\n",
    "                            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                            mae_s = mae_s + len(u_valid) * mae\n",
    "                        print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                         np.sqrt(\n",
    "                                                                                             accuracy_s / (test_length-k)),\n",
    "                                                                                         mae_s / (test_length-k)))\n",
    "\n",
    "                print (str(epoch) + ':\\n')\n",
    "                print(\"\\nEvaluation:\")\n",
    "                print (\"train:rmse,mae:\", train_rmse / ll, train_mae / ll)\n",
    "                train_rmse = 0\n",
    "                train_mae = 0\n",
    "\n",
    "                loss_s = 0\n",
    "                accuracy_s = 0\n",
    "                mae_s = 0\n",
    "\n",
    "                ll_test = int(len(test_data) / batch_size) + 1\n",
    "                for batch_num in range(ll_test):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                    data_test = test_data[start_index:end_index]\n",
    "\n",
    "                    userid_valid, itemid_valid, y_valid = zip(*data_test)\n",
    "                    u_valid = []\n",
    "                    i_valid = []\n",
    "                    for i in range(len(userid_valid)):\n",
    "                        u_valid.append(u_text[userid_valid[i][0]])\n",
    "                        i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                    u_valid = np.array(u_valid)\n",
    "                    i_valid = np.array(i_valid)\n",
    "\n",
    "                    loss, accuracy, mae = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid)\n",
    "                    loss_s = loss_s + len(u_valid) * loss\n",
    "                    accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                    mae_s = mae_s + len(u_valid) * mae\n",
    "                print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                 np.sqrt(accuracy_s / (test_length-k)),\n",
    "                                                                                 mae_s / (test_length-k)))\n",
    "                rmse = np.sqrt(accuracy_s / (test_length-k))\n",
    "                mae = mae_s / (test_length-k)\n",
    "                if best_rmse > rmse:\n",
    "                    best_rmse = rmse\n",
    "                if best_mae > mae:\n",
    "                    best_mae = mae\n",
    "                print(\"\")\n",
    "            print ('best rmse:', best_rmse)\n",
    "            print ('best mae:', best_mae)\n",
    "\n",
    "    print ('end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best RMSE using DeepCoNN is 0.9963864639159685.\n",
      "The best MAE using DeepCoNN is 0.7003501264889348.\n"
     ]
    }
   ],
   "source": [
    "print(\"The best RMSE using DeepCoNN is {}.\" .format(best_rmse))\n",
    "print(\"The best MAE using DeepCoNN is {}.\" .format(best_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
